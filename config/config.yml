# ----------------------
# Model Architecture
# ----------------------
model:
  base_encoder: resnet
  encoder_params:
    input_channels: 3
    layer_channels: [128, 256, 512, 1024]  # Channels per block stage
  projection_head:
    type: mlp
    input_dim: 1024     # Matches final encoder output
    hidden_dim: 512
    output_dim: 128     # Final projection space (contrastive space)

# ----------------------
# Training
# ----------------------
training:
  epochs: 500

  warmup_epochs: 10
  learning_rate: 0.3
  weight_decay: 1e-4
  optimizer: Adam   # For large-batch training stability. options include SGD and Adam
  momentum: 0.9
  # trust_coefficient: 0.02
  temperature: 0.5  # For NT-Xent loss

  train_batch_size: 64
  val_batch_size: 64
  
  num_workers: 4
  pin_memory: true
# ----------------------
# Data Augmentation
# ----------------------
augmentation:
  crop_size: 32
  random_resized_crop:
    scale: [0.2, 1.0]
  color_jitter:
    p: 0.8
    strength: [0.4, 0.4, 0.4, 0.1]
  grayscale_prob: 0.2
  gaussian_blur_prob: 0.5
  gaussian_blur_kernel_size: 3
  normalize:
    mean: [0.4914, 0.4822, 0.4465]
    std: [0.2023, 0.1994, 0.2010]

# ----------------------
# Miscellaneous
# ----------------------
dataset:
  name: cifar10
  path: ./data

checkpoint:
  save_dir: ./checkpoints/simclr/
  save_freq: 50

logging:
  log_dir: ./logs/
  log_interval: 10
  use_tensorboard: true

device: cpu
